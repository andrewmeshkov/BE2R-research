{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4331fc41-629c-430e-bc2d-085a69ed033c",
   "metadata": {},
   "source": [
    "# [1] Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey\n",
    "https://arxiv.org/html/2508.13073v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3d11c-fabb-44a2-b79c-c407e1e0553d",
   "metadata": {},
   "source": [
    "# [2] Early Fusion Helps Vision Language Action Models Generalize Better\n",
    "https://openreview.net/pdf/597ad7d82069689b810bb1d506f1ed3dcfbe2bc1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e7ab5f-1939-467c-b430-8ecf06f30cf0",
   "metadata": {},
   "source": [
    "Гипотеза: при помощи правильного подобранного смешивания энкодеров можно улучшить понимаю моделью совместного пространства действие-текст-изображение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf3e1fc-0d04-4371-9408-300f2850a426",
   "metadata": {},
   "source": [
    "Тут хочется рассмотреть не только методы, которые используются только для VLA, но и рассмотреть варианты заимствования подходов из исследований мультимодальности для VLM, как например посмотреть, можно ли применить что то похожее на архитектуру OmniFusion, с добавлением адаптеров под конкретные задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09379b6e-da64-4861-97c6-97dafa1653d4",
   "metadata": {},
   "source": [
    "В [2] показывается то, как можно использовать CLIP в VLA модели, однако он обучается только на основе пар изображений-текст, отсюда хочется попробовать вывести гипотезу, что можно попробовать обучать совместное представление на базе триплета изображение-действие-текст"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf264a-2712-4478-b4a8-0d19f0e13199",
   "metadata": {},
   "source": [
    "Как первую инстанцию проверки, как мне кажется, стоит рассмотреть InfoNCE на этих трех модальностей в 2ух видах, а именно: попарный ((изображение-действие, действие-текст и изображение-текст), совместный (изображение-действие-текст)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b44636-c19f-4739-b2c7-793e3c34dd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 7.7399749755859375\n",
      "Pairwise avg. loss: 3.822693427403768\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"\n",
    "Реализуем код для двух вариантов реализации конгтрастного обучения для получения совместных эмбеддингов\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def multi_modal_info_nce_loss(embeddings_list, temperature=0.1, eps=1e-8):\n",
    "     \n",
    "    M = len(embeddings_list) \n",
    "    B, D = embeddings_list[0].shape\n",
    "    for e in embeddings_list:\n",
    "        assert e.shape == (B, D)\n",
    " \n",
    "    embeddings = [F.normalize(e, dim=1) for e in embeddings_list]   \n",
    " \n",
    "    z = torch.cat(embeddings, dim=0)\n",
    "    device = z.device\n",
    "    N = z.shape[0] \n",
    "    sim = torch.matmul(z, z.T) / temperature\n",
    " \n",
    "    diag_mask = torch.eye(N, device=device).bool()\n",
    " \n",
    "    idx = torch.arange(N, device=device)\n",
    "    sample_idx = idx % B \n",
    "    same_sample = sample_idx.unsqueeze(0) == sample_idx.unsqueeze(1)   \n",
    "    positives_mask = same_sample & (~diag_mask)  \n",
    "    exp_sim = torch.exp(sim) * (~diag_mask).float() \n",
    "    numerator = (exp_sim * positives_mask.float()).sum(dim=1)  \n",
    " \n",
    "    denominator = exp_sim.sum(dim=1) + eps  \n",
    " \n",
    "    loss_i = -torch.log((numerator + eps) / denominator) \n",
    "    return loss_i.mean()\n",
    "\n",
    " \n",
    "def nt_xent_pairwise(z1, z2, temperature=0.1): \n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    logits = torch.matmul(z1, z2.T) / temperature \n",
    "    labels = torch.arange(z1.size(0), device=z1.device) \n",
    "    loss1 = F.cross_entropy(torch.cat([logits,], dim=1), labels) \n",
    "    return loss1\n",
    "\n",
    "\n",
    "    \n",
    "class DummyEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    " \n",
    "image_encoder = DummyEncoder(input_dim=2048, out_dim=256)\n",
    "text_encoder  = DummyEncoder(input_dim=768,  out_dim=256)\n",
    "state_encoder = DummyEncoder(input_dim=1024, out_dim=256)\n",
    " \n",
    "B = 32\n",
    "image_feats = torch.randn(B, 2048)\n",
    "text_feats  = torch.randn(B, 768)\n",
    "state_feats = torch.randn(B, 1024)\n",
    " \n",
    "z_img = image_encoder(image_feats)\n",
    "z_txt = text_encoder(text_feats)\n",
    "z_act = state_encoder(state_feats)\n",
    "\n",
    "loss = multi_modal_info_nce_loss([z_img, z_txt, z_act], temperature=0.07)\n",
    "loss_ta = nt_xent_pairwise(z_txt, z_act, temperature=0.07)\n",
    "loss_it = nt_xent_pairwise(z_img, z_txt, temperature=0.07)\n",
    "loss_ai = nt_xent_pairwise(z_img, z_act, temperature=0.07)\n",
    "print(\"Loss :\", loss.item())\n",
    "print(\"Pairwise avg. loss:\", (loss_ta.item() + loss_it.item() + loss_ai.item()) / 3)\n",
    "\n",
    "opt = torch.optim.Adam(list(image_encoder.parameters()) +\n",
    "                       list(text_encoder.parameters()) +\n",
    "                       list(state_encoder.parameters()), lr=1e-4)\n",
    "\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33132b48-043e-4b6a-90cf-f11400db1764",
   "metadata": {},
   "source": [
    "На этом этапе мы можем только посмотреть на то, что в общем Loss на совмещение 3 модальностей выше, чем если брать средний попарный"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead201c0-f17e-47bd-9c67-709f297585a4",
   "metadata": {},
   "source": [
    "Также есть вариант с модальными адаптерами, например как это реализовано в OmniFusion у AIRI, приведем два примера адаптеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5685a576-3d54-461a-a493-74daa87273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MLPAdapter(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, self.out_dim), nn.GELU(), nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    " \n",
    "\n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 visual_hidden_dim: int,\n",
    "                 query_dim: int,\n",
    "                 num_queries: int,\n",
    "                 transformer_hidden_dim: int,\n",
    "                 num_transformer_layers: int,\n",
    "                 num_heads: int):\n",
    "        super(QFormer, self).__init__()\n",
    "\n",
    "        # Learnable query vectors\n",
    "        self.queries = nn.Parameter(torch.randn(num_queries, query_dim))\n",
    "\n",
    "        # Linear projection from visual encoder dimension to transformer hidden dimension\n",
    "        self.visual_projection = nn.Linear(visual_hidden_dim, transformer_hidden_dim)\n",
    "\n",
    "        # Transformer encoder to process queries and visual features\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=transformer_hidden_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=4 * transformer_hidden_dim,\n",
    "                dropout=0.1,\n",
    "                activation='relu'\n",
    "            ),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "\n",
    "        # Output projection to get final query embeddings\n",
    "        self.output_projection = nn.Linear(transformer_hidden_dim, query_dim)\n",
    "\n",
    "    def forward(self, visual_features: torch.Tensor):\n",
    "        \"\"\"\n",
    "        visual_features: Tensor of shape (batch_size, num_patches, visual_hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = visual_features.size(0)\n",
    "\n",
    "        # Project visual features to the transformer hidden dimension\n",
    "        projected_visual_features = self.visual_projection(\n",
    "            visual_features)  # Shape: (batch_size, num_patches, transformer_hidden_dim)\n",
    "\n",
    "        # Repeat the queries across the batch\n",
    "        queries = self.queries.unsqueeze(0).expand(batch_size, -1, -1)  # Shape: (batch_size, num_queries, query_dim)\n",
    "\n",
    "        # Concatenate queries and visual features\n",
    "        combined_features = torch.cat([queries, projected_visual_features],\n",
    "                                      dim=1)  # Shape: (batch_size, num_queries + num_patches, transformer_hidden_dim)\n",
    "\n",
    "        # Apply the transformer\n",
    "        transformed_features = self.transformer(combined_features.permute(1, 0,\n",
    "                                                                          2))  # Shape: (num_queries + num_patches, batch_size, transformer_hidden_dim)\n",
    "        transformed_features = transformed_features.permute(1, 0,\n",
    "                                                            2)  # Shape: (batch_size, num_queries + num_patches, transformer_hidden_dim)\n",
    "\n",
    "        # Extract the transformed queries\n",
    "        transformed_queries = transformed_features[:, :self.queries.size(0),\n",
    "                              :]  # Shape: (batch_size, num_queries, transformer_hidden_dim)\n",
    "\n",
    "        # Project the transformed queries to the query dimension\n",
    "        output_queries = self.output_projection(transformed_queries)  # Shape: (batch_size, num_queries, query_dim)\n",
    "\n",
    "        return output_queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30bf12-775d-4a18-864f-21cb538161e1",
   "metadata": {},
   "source": [
    "Последним пунктом из тех экспериментов, которые интересно произвести назовем смешивание эмбеддингов при помощи взвешенного сложения, кросс attn и подобных алгоритмов, по итогу работы которых мы получаем общий вектор представления для двух модальностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de99a40d-bf64-4e3d-ae61-4733c7e8ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedSumMixer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight_1 = nn.Parameter(torch.randn(1,))\n",
    "        self.weight_2 = nn.Parameter(torch.randn(1,))\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.weight_1 * x1 + self.weight_2 + x2\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78ab82-b2cd-4665-ae97-eb71924331ab",
   "metadata": {},
   "source": [
    "Итогом проведения этих экспериментов в идеале стала бы статистика того, насколько нам может помочь смешивание эмбеддингов в VLA, какие идеи можно позаимствовать и как сделать устойчивое пространства общего представления текст-действие-изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950607ba-0098-4f75-8c5d-9928e212c759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
